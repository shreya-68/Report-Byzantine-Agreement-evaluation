
\section{Introduction}

The Distributed Consensus problem introduced by Pease et al.
\cite{PeaseSL80} is one of the most important and well-studied problems in
distributed systems. In essence, the problem deals with multiple processes,
each with an initial opinion, cooperating with each other to reach agreement.
The motivation for this problem arises from real-world use cases such as database
transactions, where a number of systems need to agree on whether to commit or
abort a transaction, or in aircraft controllers, for example, the Dragon flight system~\cite{dfs}, that need to agree on which
plane should take-off or land first. In the case of the latter, where the
system is safety-critical, it is essential that the aircraft controllers reach
an agreement within a bounded period of time and that all the controllers
arrive at the same decision, whereas in the case of the former, it is only
necessary that the system eventually reaches an agreement. Therefore, any
consensus reaching algorithm needs to satisfy certain correctness conditions
that are formally given as follows \cite{PeaseSL80}:

\begin{itemize} \item \textit{Consistency}: All correct processes agree on the
        same value and all the decisions are final.  
    \item \textit{Validity}:
        If all processes start with the same initial value $v$, then $v$ is the
    only allowable decision for all processes.  
\item \textit{Termination}: All
        correct processes reach a decision.  \end{itemize}

%TODO: give example of synchronous

These correctness conditions also define the \textit{safety} and \textit{liveness}
conditions that a distributed consensus algorithm must satisfy.  The
consistency and validity condition is a safety condition: safety will be
violated if any two processes decide on different values. The termination
condition specifies the liveness condition. For a system to continue executing
correctly until it terminates, all processes must eventually reach the same
conclusion.

In real-world applications, it would be unrealistic to assume that the systems
involved in solving the problem will continue to work correctly, for the whole
duration, as specified by the protocol. There may be cases such as when the
communication links between systems break and the system \textit{partitions},
the network becomes congested due to too many requests, or the messages are not
delivered in order. To achieve reliability in distributed systems, protocols
are needed which enable the system as a whole to continue to function despite
the failure of some number of components. 

It was shown by Lamport et al.\ that consensus can never be reached even
between two processes if there is a link failure~\cite{LamportSP82}. Under the
assumption that the links do not display any kind of fault, one needs to
analyze scenarios in the case of process failures. Generally, the kind of
failures that a process can encounter is either a \textit{fail-stop} failure or
a \textit{Byzantine failure}. Fail-stop failures occur when processes fail by
stopping. In the case of a byzantine failure, the processes fail by acting
maliciously. Depending on the ratio of faulty processes to all the processes,
consensus may be impossible even when the system is synchronous. The problem of
solving consensus in the presence of byzantine failures is known as the
Byzantine Agreement problem~\cite{LamportSP82}. With the increasing number of
malicious attacks reported in recent times and software bugs leading to
processes behaving arbitrarily, dealing with this problem has become more
important than ever before.

\subsection{The Need for Experimental Evaluation}

In the past, researchers have tried to optimize either the \textit{message}
complexity or the \textit{round} complexity of algorithms in the worst case
scenarios. Worst-case bounds are insufficient to provide an insight into the
tradeoffs that exist between the two under various conditions such as varying 
network size and, percentage of faults. However, experimental evaluation 
can provide detailed results on guarantees of algorithms under
such varying conditions.  While the literature is rich in theoretical results,
there has been a lack of extensive practical results for these algorithms. This
can be largely attributed to the fact that simple algorithms are theoretically
inefficient while the more complex ones are hard to implement. Theoretical
results give us information about the worst case scenarios but in
practice, the system does not always run in the worst case. 

In real-world systems, certain constraints---for example, communication
ban\-d\-widths---may exist which are not considered in theoretical results. Such
constraints can reduce the performance---if the available bandwidth is
small---or can be used to our advantage by tweaking the algorithm---sending
multiple packets together instead of in separate rounds. Furthermore, analytical 
techniques using the Big `O' notation can hide large constants which are
of significance when we look at memory or bandwidth consumption in a real-world
system. Many complex algorithms make certain assumptions about systems, for
example, the existence of a large fraction of faulty processes. However, in practice, 
only a small fraction of processes are faulty and in such cases the
algorithm would be unnecessarily complex and inefficient.


Analysis of an algorithm under such varying conditions, and understanding the
performance under practical constraints is an incentive for experimental
evaluation of theoretically well-performing algorithms. While some applications
require best performance for either only number of bits required for
communication or for the number of rounds taken to reach consensus, most
applications \todo{add citation} require optimal results for both---size
of and number of faults in the application. A comparison, therefore, 
of best-performing results is necessary to understand how algorithms perform with respect to 
each other. A way to achieve this is by selecting state-of-the-art algorithms having different design criteria and implementing them on a common platform.

Motivated by these needs, we evaluate the performance of three recently
proposed solutions for the Byzantine Agreement problem. Two of these are
randomized algorithms - (1) by Ben-Or et al. \cite{BPV06}, which we call
algorithm \textit{Quorum} throughout the paper, and (2) an almost-everywhere to
everywhere algorithm by Braud-Santoni et al.  \cite{BGH13} combined with the
almost-everywhere algorithm of \cite{KSSV06}, which we call algorithm
\textit{Pull-Push}. An almost-everywhere algorithm reaches consensus among all
but a fraction of the processes and an almost-everywhere to everywhere
algorithm reaches consensus among all the processes. 
The third is a deterministic algorithm, due to Kowalski et
al. \cite{KM13}, which we refer to as algorithm \textit{EIG} (Exponential
Information Gathering). 

\subsection{Motivation for the Chosen Algorithms}

A vast amount of work has been done over the years to solve the Byzantine
Agreement problem, and different approaches have been taken for various models
of the problem. In a well-known result, Fischer et al. \cite{FischerLP83}
showed that reaching distributed consensus deterministically becomes impossible
in an asynchronous system with even just one faulty process. Randomized
algorithms allow us to overcome the barrier of this impossibility result in the
asynchronous setting. Even in the synchronous setting, they provide major
improvements over the deterministic algorithms by a factor polynomial in the
size of the network. The probabilistic techniques provide an advantage - the
worst-case scenarios can be eliminated by giving them a probability of $0$ and
a probability distribution is given to the other cases. In cyber-physical systems, 
logical synchrony is crucial as in the case of a flight system where the supervisory control is a global computation~\cite{SASMO}.
Many of the reactive systems that are safety-critical require determinism which is the target of synchronous programs\todo{Add citations}.


Algorithm
\textit{Pull-Push}, is one of the most recent and best-known result in terms of
communication complexity. The main contribution of the paper has been to give
an almost-everywhere to everywhere solution, which improves the amortized
communication complexity to $\tilde{O}(1)$ per node\footnote{$\tilde{O}$ is
same as $O$ up to a poly-logarithmic factor.}. The previously best-known result
was $\tilde{O}(n^{1/2})$ \cite{KLST11}.  The algorithm uses the model
previously considered in \cite{KLST11,KSSV06,BPV06,KS09}, that of
a synchronous, full-information model, with a non-adaptive adversary. They also
show that similar results can be shown even in the case of an asynchronous
model. This achievement in reducing the communication cost, however, comes at
the cost of a higher round complexity, which is poly-logarithmic in the size of
the network. 

This trade-off between communication and round complexity
motivated us to pick the algorithm \textit{quorum} for comparison. Algorithm
\textit{quorum}, due to Ben-Or et al., has a round complexity of $O(logn)$
although it shows a quasi-polynomial communication complexity of $n^{O(logn)}$.
It uses the same setting as \textit{Pull-Push} and makes use of the well-known
Feige's protocol of collective coin-flipping to decide on a small committee
with the same fraction of good processes as in the whole network, which then
runs a leader election protocol for agreement. A comparison of these two
algorithms will allow us to understand how they perform with respect to each 
other for both communication as well as round complexity. 

While both these randomized algorithms attempt to improve upon the metrics for
performance comparison, they consider worst-case scenarios when the fraction of
adversarial processes is $n/3$ and $n/4$, respectively. On the other hand, in
a real-world system, this fraction might actually be really small. This
motivated us to choose a deterministic algorithm for comparison which considers
this fraction during the execution of its algorithm and thus provides much
better results for both communication and round complexity when the number of
adversarial nodes to the size of the network is really small. \textit{EIG} is
a recent deterministic algorithm, more efficient than the one by Garay et
al.~\cite{GarayM98} that meets the bounds on the optimal range of Byzantine
processes and communication rounds. Further, we make algorithmic modifications
to this algorithm---removing the redundant information being sent in every round
regarding the list of byzantine nodes---to show an improvement in the
communication complexity. 


We vary two parameters to define the workload of the system---the number of
processes, and with that the fraction of faults in the system.  For evaluation,
we use three metrics: communication complexity and round complexity, which are
commonly used for theoretical evaluation, and latency, which is an important
measure of efficiency for empirical results. We report on the behavior of the
algorithms under varying parameters and analyze their effectiveness using
these metrics.  These experimental results will allow us to determine the best
algorithms to pick given the workload and requirements of a system,
i.e., the fraction of adversarial nodes, the bandwidth requirements and
latency. For example, in certain scenarios with low adversarial nodes, the
deterministic algorithm performs better than both the other ones.

Section~\ref{sec:background} gives an introduction to the various models used
to solve the distributed consensus problem and provides a summary of related
work in this field. In Section~\ref{sec:algos}, we elaborate on the three
algorithms under comparison. Section~\ref{sec:eval} and \ref{sec:results}
details the implementation and evaluates the results obtained from our
experimental evaluation. This is followed by a discussion that includes
implementation techniques for better performance, and Section~\ref{sec:conc}
concludes the paper.
