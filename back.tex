\section{Background}
\label{sec:background}

\subsection{Problem statement}
The problem of Byzantine Agreement, in its most basic form is defined as follows \cite{PeaseSL80}:

\begin{definition}
Let $\mathcal{P}$ be a protocol among $n$ processes $P = \{ P_1, P_2, \dots, P_n\}$, such that $B \subset P$ processes are byzantine. Each process $P_i$ starts with an input bit $b_i$, and $P_i$ outputs a bit $c_i$ at the end of the protocol. $P$ is a Byzantine Agreement Protocol, if the following conditions hold:
\begin{itemize}
    \item \textit{Consistency}: For any two non-faulty processes $P_i$ and $P_j \in P \backslash B$, $c_i = c_j$.
    \item \textit{Validity}: If $b_i = b$ for all non-faulty processes $P_i \in P \backslash B$, then $c_i = b$ for all non-faulty processes $P_i$.
    \item \textit{Termination}: Protocol $\mathcal{P}$ terminates with probability $1$.
\end{itemize}
\end{definition}

A protocol is said to be $k$-fault tolerant if it operates correctly as long as no more than $k$ processes fail during execution. The following theorem by \cite{LamportSP82,PeaseSL80} shows the impossibility result when $k \geq n/3$.

\begin{theorem}
There is a $k$-fault tolerant synchronous protocol to solve the byzantine agreement problem iff $k < n/3$.
\end{theorem}

\subsection{Complexity measures}

The practicality of agreement protocols depends heavily on their computational complexity. Theoretically, when talking about complexity measures of algorithms for distributed consensus, one generally uses the following two metrics:
\begin{itemize}
\item \textit{Round} complexity, that is the number of rounds of message exchange before all the non-faulty processes decide.
\item \textit{Communication} complexity, that is the total number of messages sent per process or the total number of bits sent per process.
\end{itemize}
For our purposes, we will also consider the following metric: 
\begin{itemize}
\item \textit{Latency}, is the overall CPU time utilization from start till all the non-faulty processes decide.
%\item \textit{Memory} comsumption, is the memory requirement for the execution of an alorithm for a configuration and depends mainly upon the data structures used for storing of values.
\end{itemize}
All of these quantities are in general dependent on which faults occur and when, and how parallel the algorithm is. As the size of the system increases, i.e., the number of processes increase, the expected number of faults grows linearly with it. Also, for example, a highly parallel system might require more memory but reduces the round complexity. Round complexity is also an indication of how much of the algorithm can be executed in parallel.

\subsection{Previous work}

\subsubsection{Deterministic Solutions}
Fischer and Lynch \cite{Fischer81alower} proved that $k + 1$ is the round complexity in the worst case for a $k$-fault tolerant synchronous protocol. If the messages were not \textit{authenticated}, the message complexity was initially shown to be exponential in the number of process in \cite{PeaseSL80}. In 1998, Garay and Moses \cite{GarayM98}, with modifications to the two phase protocol of \cite{BDDS87} using the EIG data structure, improved the message complexity further to polynomial time. If authenticated messages were sent, Dolev and Reischuk \cite{DolevR85}, proposed an algorithm using $O(n + k^2)$ messages. In an attempt to lower the communication costs, researchers either lowered the fraction of faulty processes to a smaller number \cite{DRS90} or increased the maximum number of rounds needed in the worst case \cite{TPS87}. It was only recently that Kowalski et al. \cite{KM13} proposed a simple algorithm that holds for the optimal range and optimal number of communication rounds while lowering the communication complexity to $O(n^{3}logn)$.

\subsubsection{Randomized Solutions}
Probabilistic solutions were proposed to circumvent the lower bounds on round and message complexity imposed by deterministic settings. They used the idea of a common coin, which was seen as `sufficiently random' by `sufficiently many random' processes. 
In the asynchronous setting, using randomized algorithms, Ben-Or \cite{Ben-Or83} showed that if $k < n/5$, then consensus is achievable with probability $1$. Rabin \cite{Rabin83} showed constant expected round complexity if $k < n/4$. Greatly improved results have been shown in \cite{PCR14, KKKSS08, MHR14} for non-adaptive adversary and in \cite{KS13,AAKS14} for an adaptive adversary. Assuming that communication channels are \textit{private} between every pair of processes, the algorithm proposed in \cite{PR10} shows constant expected round complexity and $\tilde{O}(n^2)$ message complexity. These bounds are also applicable to the asynchronous setting. In \cite{HKK08}, the authors proved an $\Omega(\sqrt[3]{n})$ lower bound on both message complexity and round complexities for synchronous systems, under some restrictive assumptions.

\subsubsection{Almost-everywhere Solutions}
The almost-everywhere byzantine agreement problem, was introduced by Dwork et al. in \cite{DPPU88}. It is a relaxed version of the byzantine agreement problem and requires all but an $O(log^{-1}n)$ fraction of the processes to agree on a common output. For the algorithm in \cite{KSSV06}, the round and message complexities are shown to be poly-logarithmic in $n$. Construction of byzantine agreement from almost-everywhere byzantine agreement, called almost-everywhere reduction, was proposed in \cite{KS09,KLST11}, using $\tilde{O}(\sqrt{n})$ bits per process and poly-logarithmic number of rounds. 
Papers like \cite{KLST11} used push-pull protocols, the complexity of which is dictated by the complexity of the first \textit{push} phase and the size of the candidate lists, i.e., the number of all possible outputs. The authors of \cite{BGH13} propose an almost-everywhere to everywhere solution using the almost-everywhere algorithm of \cite{KSSV06}.

%\begin{table}[h]
%    \resizebox{\columnwidth}{!}{%
%        \begin{tabular}{lllll}
%    \hline
%    %\multicolumn{5}{c}{Deterministic} \\
%\hline
%\textbf{Algorithm} & \textbf{n} & \textbf{Round Complexity} & \textbf{Bit Complexity per node} \\ \hline
%Pease et. al (1980)      & $3k + 1$     & $k + 1$                     & $exp(n)$     \\
%Garay and Moses (1998)   & $3k + 1$     & $k + 1$                     & $O(n^9)$     \\
%Kowalski et. al (2013)   & $3k + 1$     & $k + 1$                     & $O(n^3logn)$ \\ \hline
%
%\end{tabular}
%}
%\end{table}
%
%\begin{table}[h]
%    %\resizebox{\columnwidth}{!}{%
%\begin{tabular}{llllll}
%    \hline
%    \multicolumn{6}{c}{Randomized} \\
%\hline
%\textbf{Algorithm} & Adaptive/Non-adaptive & \textbf{n} & \textbf{Rounds} & \textbf{Bit Complexity} & \textbf{Trade-off} \\ \hline
%Ben-Or, Pavlov et. al (2006) & Non-adaptive & $4k + 1$ & $O(logn)$ & $n^{O(logn)}$         & Everywhere BA     \\
%        King and Saia (2009) & Non-adaptive & $3k + 1$ & Polylog & $\tilde{O}(n^{3/2})$    & Almost everywhere to everywhere\\
%King, Lonargan et. al (2011) & Non-adaptive & $3k + 1$ & Polylog & $\tilde{O}(\sqrt{n})$   & Almost everywhere to everywhere\\
% Braud-Santoni et. al (2014) & Non-adaptive & $3k + 1$ & Constant & $\tilde{o}(n)$         & Almost everywhere to everywhere     \\
%        King and Saia (2011) & Adaptive & $3k + 1$ & Polylog & $\tilde{O}(\sqrt{n})$ & Existence of private channels\\
%\end{tabular}
%%}
%\end{table}


\subsubsection{Experimental Evaluations}
Many surveys have been done reporting the various theoretical results for the Byzantine Agreement problem. In a recent paper, by Bruno Vavala \cite{VN12}, that implements Bracha's algorithm \cite{Bracha84} to bridge the gap between theory and practice, the authors reported that the literature is poor in the experimental evaluations of byzantine agreement algorithms. They showed that Bracha's algorithm terminates in constant rounds if only crash failures occur and under normal conditions, whereas theoretically it takes exponential number of rounds to terminate due to the worst-case scenario. They use an averaging method, approximations and stochastic techniques for analysis of the protocol. They ran the experiments for up to 100 processes and reported the round complexity results. In \cite{LSV12}, Liang et al. implemented and analyzed three different byzantine broadcast algorithms for fault-tolerant state machine protocols - 1) the classic solution by Pease et al. \cite{PeaseSL80}, 2) practical BFT protocol by Castro and Liskov \cite{CL02}, and 3) a network coding based BFT that they propose in the paper. For state replication protocols, it is important for processes to agree upon an order to process the requests. In this experimental evaluation, the authors concentrated on implementation and analysis of byzantine broadcast part of the algorithm that is used to reach consensus on the order of requests to be processed by the state machines. They reported the latency when the batch size of the requests to $4$ servers is varied. Oluwasanmi, Saia, and King \cite{OSK10} improve upon the algorithm given in \cite{KS09} which was shown to be impractical when implemented due to large hidden constants, although, they weaken the control of the adversary to only $1/8$ fraction of the processes. They implement and compare their algorithm with Cachin et al.'s \cite{CKS05} with the size of the network simulated between $1,000$ to $4,000,000$ processors. They used average number of messages and bits sent per process as well as latency for comparison.

Most of these experimental evaluations have either used a synchronous model or a partially synchronous model for simplicity. The algorithms we have chosen allow us to differentiate and compare different randomized as well as deterministic algorithms which makes it necessary to use a synchronous setting due to the impossibility results for the asynchronous case. 

