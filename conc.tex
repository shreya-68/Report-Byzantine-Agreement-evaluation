

\subsection{Discussion}

In real-world applications, achieving distributed consensus has become increasingly important. In most cases, consensus is only a small but frequently used sub-component of a larger protocol such as in distributed databases. Thus, improving the communication and round complexity of this algorithm is essential to the overall performance of such systems. One needs to consider implementation issues that come along with any of these algorithms and not only their theoretical results. During implementation, one can improve certain tasks such as sending a larger message instead of many smaller messages or executing parallelizable tasks at the same time on multi-core machines.  

As can be seen from the figures above, according to the requirements and resources available, each algorithm gives different performance. Once we have an algorithm to implement, the first thing to consider before implementation is what resources we have and how can we design the system such that we get optimal results for the available resources. Implementing the testbed framework on a cluster allowed us to mimic real-time scenarios. We considered different byzantine behaviors and during execution if any good process crashed due to unknown reasons we included it as a byzantine process. To report on the maximum number of bits sent per process, we considered byzantine processes that were trying to flood the network. On executing the algorithms in the case of a denial of service attack, an increasing ratio of faulty processes reduced the communication overhead instead of increasing it. Crashing failures also reported similar results. If the faulty processes only sent out arbitrary messages, the algorithms executed correctly without hampering the communication cost too much if the ratio of byzantine processes was increased.

Another optimization that would have save the running time of the system would be to use UDP connections instead of TCP since they take up less resources and provide lesser overhead. Even though UDP connections do not guarantee message delivery, for a small system it could be considered highly reliable still. The rationale behind using TCP connections for this testbed framework was to model and understand implementation issues for more realistic distributed applications, which could have peers separated by geographical distance. In such cases, TCP connections provide greater reliability. 

An analysis of the three algorithms and their performance for a wide range of number of processes and faults shows that communication of each process with fewer number of processes yields good results instead of all-to-all communication. This inhibits byzantine processes from influencing values of too many good processes. It is also important that requests from byzantine processes be throttled at an early stage. The good performance of the deterministic algorithm for small fault ratios shows that it is important to consider this factor when designing an algorithm.  Communication between multiple sets of quorums allows parallel tasks to be executed and gives good latency results. The combined use of these techniques might help us design improved algorithms in the future to solve the problem of distributed consensus. 


\section{Conclusion}
\label{sec:conc}

In this paper, we focused on analyzing and comparing the performance of three algorithms - an Exponential Information Gathering protocol for consensus \cite{KM13} (algorithm EIG), a randomized byzantine agreement protocol which uses the concept of \textit{quorums} to filter requests \cite{BPV06} (algorithm Quorum), and a fast byzantine agreement algorithm that uses the Pull-Push model of communication \cite{BGH13} (algorithm Pull-Push). We considered three metrics for comparison: (1) \textit{communication} complexity, (2) \textit{round} complexity, and (3) \textit{time} complexity. 

We showed that in general the algorithm Pull-Push performed better than the other two in terms of communication complexity. For increasing network sizes the communication complexity remains the same if the increase is less than double. But, this performance comes with a trade-off. When time complexity was considered, as can be seen in Fig. \ref{fig:elapsed}, algorithm Quorum performed better. This comes with the assumption that certain rounds or sub-protocols in algorithm Quorum could be executed in parallel. However, this good performance of Quorum was not reflective in its communication complexity, and it showed poor performance on comparison with algorithm Pull-Push. In real-time situations, as the number of processes in a network increase the probability of having faulty nodes in the network naturally increases. Hence, even though algorithm EIG shows better performance when the fault ratio is small, one should keep in mind the possibility of high ratio of faults in the future.

